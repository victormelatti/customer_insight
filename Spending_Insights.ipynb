{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use natural language to query database with financial transactions\n",
    "\n",
    "#### Purpose:\n",
    "Use natural language to filter the database and extract data that is then displayed on the app screen in a dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import openai\n",
    "from datetime import datetime\n",
    "import json\n",
    "from langchain.prompts import PromptTemplate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import concurrent.futures\n",
    "import warnings\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI API key. The API key is provided in the email and has a 5$ credit to start with.\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"--------- INSERT YOUR OPENAI API KEY HERE ---------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables and paths\n",
    "\n",
    "working_directory = os.getcwd()\n",
    "images_path = os.path.join(working_directory, 'images')\n",
    "llm_model = \"gpt-4-turbo\"\n",
    "db_name = \"Database_transactions.db\"\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Get today's date in YYYY-MM-DD format\n",
    "today_date = datetime(2025, 3, 4).strftime(\"%Y-%m-%d\")\n",
    "day_of_week = datetime(2025, 3, 4).strftime(\"%A\")\n",
    "preprocessing_time_threshold = 4  # a threshold for preprocessing time in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_name, dpi = 300, scale = 1.5):\n",
    "    # Load an image in notebook\n",
    "\n",
    "    image_path = os.path.join(images_path, image_name)\n",
    "    \n",
    "    img = mpimg.imread(image_path)\n",
    "    height, width = img.shape[:2]  # Extract height & width\n",
    "\n",
    "    fig_width = width / dpi  # Calculate figure width in inches\n",
    "    fig_height = height / dpi  # Calculate figure height in inches\n",
    "\n",
    "    plt.figure(figsize=(fig_width * scale, fig_height* scale), dpi=dpi)  \n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "# Timing Decorator\n",
    "def timing_decorator(func):\n",
    "    \"\"\"Decorator to measure execution time of a function.\"\"\"\n",
    "    \n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Start timer\n",
    "        result = func(*args, **kwargs)  # Execute the function\n",
    "        end_time = time.time()  # End timer\n",
    "        execution_time = end_time - start_time\n",
    "        #print(f\"âœ… {func.__name__} executed in {execution_time:.4f} seconds\")\n",
    "        return result, execution_time\n",
    "    return wrapper\n",
    "\n",
    "# validation function\n",
    "def validation(df_testing_results, benchmark_file = \"benchmarks.csv\"):\n",
    "    \n",
    "    # Load the results from the CSV file\n",
    "    benchmarks = pd.read_csv(benchmark_file)\n",
    "\n",
    "    benchmarks[\"Response\"] = benchmarks[\"Response\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else None)\n",
    "    comparison_df = df_testing_results.merge(benchmarks[['Original Query', 'Response']], left_on=\"Original Query\", right_on=\"Original Query\", suffixes=('_test', '_benchmark'))\n",
    "\n",
    "    comparison_df['PASSED TEST?'] = comparison_df['Response_test'].eq(comparison_df['Response_benchmark'])\n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SQLite database is created with two tables:\n",
    "1) Transactions Table: the user's transactions (it is assumed that the transactions table is already filtered for a single user)\n",
    "2) Merchants Table: the spending category linked to each Merchant (a 1-1 relation is assumed)\n",
    "\n",
    "An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image('test_set.png', dpi = 300, scale = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The design the process\n",
    "\n",
    "- a user asks a question about financial transactions\n",
    "- the query is processed, checking for spelling mistakes and maps activities to a finite set of categories (es food shopping -> Groceries)\n",
    "- relevant features are exracted from the preprocessed query \n",
    "- the extracted features populate a predefined SQL query\n",
    "- a structured database is called using the generated SQL query\n",
    "- The results are returned\n",
    "\n",
    "The process is shown in the image below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image('process_design.png', dpi = 300, scale = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create a SQLite database with a transactions table and a merchant table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_sqlite(db_name=db_name):\n",
    "    \"\"\"\n",
    "    Creates a SQLite database with a transactions table.\n",
    "    If the table already exists, it is dropped and recreated with fresh data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Connect to SQLite database (creates the file if it doesn't exist)\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Drop the table if it already exists\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS transactions\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS def_merchants\")\n",
    "\n",
    "    # Define the SQL query to create a new table\n",
    "    create_transactions_table_query = \"\"\"\n",
    "    CREATE TABLE transactions (\n",
    "        transaction_id TEXT PRIMARY KEY,\n",
    "        date TEXT NOT NULL,\n",
    "        amount REAL NOT NULL,\n",
    "        merchant_id TEXT NOT NULL\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    create_def_table_query = \"\"\"\n",
    "    CREATE TABLE def_merchants (\n",
    "        merchant_id TEXT PRIMARY KEY,\n",
    "        merchant TEXT NOT NULL,\n",
    "        spending_cat TEXT NOT NULL\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query to create the table\n",
    "    cursor.execute(create_transactions_table_query)\n",
    "\n",
    "    cursor.execute(create_def_table_query)\n",
    "\n",
    "    # Define transaction data\n",
    "    transactions = [\n",
    "        ('T-00', '2023-06-15', 35.0, \"M-01\"),\n",
    "        ('T-01', '2023-07-20', 50.0, \"M-04\"),\n",
    "        ('T-02', '2023-09-05', 75.0, \"M-05\"),\n",
    "        ('T-03', '2024-08-01', 43, \"M-07\"),\n",
    "        ('T-04', '2024-08-01', 45.0, \"M-03\"),\n",
    "        ('T-05', '2024-08-31', 49, \"M-02\"),\n",
    "        ('T-06', '2024-08-31', 50.0, \"M-03\"),\n",
    "        ('T-07', '2024-09-30', 55, \"M-01\"),\n",
    "        ('T-08', '2024-09-30', 55.0, \"M-04\"),\n",
    "        ('T-09', '2024-10-30', 61, \"M-01\"),\n",
    "        ('T-10', '2024-10-30', 60.0, \"M-03\"),\n",
    "        ('T-11', '2024-11-29', 67, \"M-01\"),\n",
    "        ('T-12', '2024-11-29', 65.0, \"M-03\"),\n",
    "        ('T-13', '2024-12-29', 73, \"M-02\"),\n",
    "        ('T-14', '2024-12-29', 70.0, \"M-03\"),\n",
    "        ('T-15', '2025-01-28', 79, \"M-01\"),\n",
    "        ('T-16', '2025-01-28', 75.0, \"M-06\"),\n",
    "        ('T-17', '2025-02-27', 85, \"M-07\"),\n",
    "        ('T-18', '2025-02-27', 80.0, \"M-03\"),\n",
    "        ('T-19', '2025-03-01', 20.0, \"M-04\"),\n",
    "        ('T-20', '2025-03-01', 10.0, \"M-01\"),\n",
    "        ('T-21', '2025-03-01', 40.0, \"M-06\"),\n",
    "        ('T-22', '2025-03-02', 10.0, \"M-01\"),\n",
    "        ('T-23', '2025-03-03', 60.0, \"M-05\"),\n",
    "        ('T-24', '2025-03-03', 100.0, \"M-04\"),\n",
    "        ('T-25', '2025-03-04', 40.0, \"M-08\"),\n",
    "        ('T-26', '2025-03-04', 10.0, \"M-06\")]\n",
    "\n",
    "    def_merchants = [(\"M-01\", \"TESCO\", \"GROCERIES\"), \n",
    "                 (\"M-02\", \"WAITROSE\", \"GROCERIES\"), \n",
    "                 (\"M-03\", \"VUE\", \"ENTERTAINMENT\"), \n",
    "                 (\"M-04\", \"STARLING\", \"FINANCE\"), \n",
    "                 (\"M-05\", \"SKY\", \"BILLS\"), \n",
    "                 (\"M-06\", \"MCDONALD'S\", \"EATING OUT\"),\n",
    "                 (\"M-07\", \"STARBUCKS\", \"COFFEE\"),\n",
    "                  (\"M-08\", \"MONZO\", \"FINANCE\")]\n",
    "\n",
    "    # Insert transaction data into the table\n",
    "    cursor.executemany(\"INSERT INTO transactions VALUES (?, ?, ?, ?)\", transactions)\n",
    "\n",
    "    cursor.executemany(\"INSERT INTO def_merchants VALUES (?, ?, ?)\", def_merchants)\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Database '{db_name}' created successfully with 'transactions' table rewritten.\")\n",
    "\n",
    "\n",
    "    query = \"SELECT transaction_id, date, amount, spending_cat, merchant FROM transactions LEFT JOIN def_merchants ON transactions.merchant_id = def_merchants.merchant_id ;\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all results\n",
    "    transactions = cursor.fetchall()\n",
    "    first_transaction_date = transactions[0][1]\n",
    "    # Print results\n",
    "    print(\"\\n\\n\\nDisplaying table: \\n\")\n",
    "    #print(\"Merchant ID | Merchant       |Spending Category\")  \n",
    "    print(\"Transaction ID | Date       | Amount | Spending Category | Merchant\")  \n",
    "    print(\"-\" * 70)\n",
    "    for transaction in transactions:\n",
    "        #print(f\"{transaction[0]:<11} | {transaction[1]:<14} | {transaction[2]:<6}\")\n",
    "        print(f\"{transaction[0]:<14} | {transaction[1]:<10} | {transaction[2]:<6} | {transaction[3]:<18}| {transaction[4]:<10}\")\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    return first_transaction_date\n",
    "\n",
    "first_transaction_date = create_db_sqlite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Define the functions and mappings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined spending categories and merchants for normalization\n",
    "CATEGORY_MAPPING = {\n",
    "    \"GROCERIES\": [\"food shop\", \"supermarket\", \"shopping\"],\n",
    "    \"ENTERTAINMENT\": [\"movies\", \"cinema\", \"fun\", \"leisure\"],\n",
    "    \"EATING OUT\": [\"restaurant\", \"dining\", \"takeaway\"],\n",
    "    \"COFFEE\": [\"cafe\", \"starbucks\", \"coffee shop\"],\n",
    "    \"FINANCE\": [\"bank\", \"investment\"],\n",
    "    \"BILLS\": [\"utilities\", \"electricity\", \"internet\"]\n",
    "}\n",
    "\n",
    "function_calling = [\n",
    "    {\n",
    "        \"name\": \"extract_query_features\",\n",
    "        \"description\": \"Extracts key features from a user query related to transactions, including date ranges (formatted as YYYY-MM-DD), merchant names, spending categories, and the required SQL aggregation function.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"start_date\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"format\": \"date\",\n",
    "                    \"description\": f\"\"\"The start date extracted from the query (YYYY-MM-DD). If the query specifies a relative time frame (e.g., 'last week', 'last 4 months'), compute the actual date based on today's date  (Today is {today_date}, {day_of_week}).If only a month is mentioned, assume the first day of that month and the current year from today date.\n",
    "                                    If the query does not specify a start date, use the earliest date available in the database, namely {first_transaction_date}. the week starts on Monday and ends on Sunday. If the date format is ambiguous, assume it follows the DD-MM-YYYY format\"\"\"\n",
    "                },\n",
    "                \"end_date\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"format\": \"date\",\n",
    "                    \"description\": f\"\"\"The end date extracted from the query (YYYY-MM-DD). If the query specifies a relative time frame, compute the actual date based on today's date (Today is {today_date}, {day_of_week}). If only a month is mentioned, assume the last day of that month and the current year from today date.\n",
    "                    If the date format is ambiguous, assume it follows the DD-MM-YYYY format.\"\"\"\n",
    "                },\n",
    "                \"merchant\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"TESCO\", \"WAITROSE\", \"VUE\", \"STARLING\", \"SKY\", \"MCDONALD'S\", \"STARBUCKS\", \"MONZO\"],\n",
    "                    \"nullable\": True,\n",
    "                    \"description\": \"\"\"The merchant name mentioned in the query, if present. For example, \"McDonald's\". If no merchant is specified, this should be null.\"\"\"\n",
    "                },\n",
    "                \"spending_category\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": ['GROCERIES', 'ENTERTAINMENT', 'EATING OUT', 'COFFEE', 'FINANCE', 'BILLS'],\n",
    "                    \"nullable\": True,\n",
    "                    \"description\": \"The spending category mentioned in the query, if present. If no category is mentioned but only a merchant, this must be null.\"\n",
    "                },\n",
    "                \"sql_aggregation\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"SUM\", \"AVG\", \"COUNT\", \"MAX\", \"MIN\"],\n",
    "                    \"nullable\": True,\n",
    "                    \"description\": \"The SQL aggregation function required to answer the query. Example: 'SUM' for total spend, 'COUNT' for the number of transactions.\"\n",
    "                    \"If no aggregation function is needed and the user wants to see its transations, this must be null.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"start_date\", \"end_date\", \"sql_aggregation\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing_decorator\n",
    "def preprocess_user_query(user_query):\n",
    "\n",
    "    \"\"\"\n",
    "    Uses OpenAI to correct spelling, normalize language, and make the query more structured.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_preprocess = PromptTemplate.from_template(\"\"\"\n",
    "    Given a user query about their transactions, perform these two tasks to preprocess the query for further analysis:\n",
    "                                                     \n",
    "        Task 1: correct spelling and grammar while ensuring key financial terms remain intact. If a merchant name is misspelled, replace with the correct spelling.\n",
    "                Example input: \"how muc i spnd at fod sop at Weitros in lastt 3 months?\"\n",
    "                Example output: \"How much did I spend at food shop at Waitrose in the last 3 months?\"\n",
    "\n",
    "                Example input: \"how much i spnd at Mcdonald two month go?\"\n",
    "                Example output: \"How much did I spend at McDonald's two months ago?\"\n",
    "                                                     \n",
    "        Task 2: if a spending category is mentioned, normalize it to a standard category.\n",
    "                Use this mapping structure to map user input to **ONLY** these standard categories GROCERIES, ENTERTAINMENT, EATING OUT, COFFEE, FINANCE, BILLS. \n",
    "                {CATEGORY_MAPPING}\n",
    "                \n",
    "                NOTE that the values of the provided mapping are not exhaustive lists of synonyms for each standard categories.\n",
    "\n",
    "                Example input:  \"How much did I spend at food shop at Waitrose in the last 3 months?\"\n",
    "                Example output: \"How much did I spend in GROCERIES at Waitrose in the last 3 months?\"\n",
    "                                                     \n",
    "                Example input:  \"How much did I spend for brunch from 01/12/2024 to 01/01/2025?\"\n",
    "                Example output: \"How much did I spend in EATING OUT for brunch from 01/12/2024 to 01/01/2025?\"\n",
    "    \n",
    "    User query: \"{user_query}\"\n",
    "                                                     \n",
    "    Return only the query after processing the above tasks. Do not include any other text in your answer\n",
    "    Normalized query:\n",
    "    \"\"\")\n",
    "\n",
    "    prompt = prompt_preprocess.format( user_query=user_query, CATEGORY_MAPPING = CATEGORY_MAPPING)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = llm_model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    norm_query = response.choices[0].message.content.strip()\n",
    "    \n",
    "    return norm_query\n",
    "\n",
    "\n",
    "def generate_sql_query(start_date, end_date, merchant, spending_category, sql_aggregation):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates an SQL query based on the extracted user's query parameters.\n",
    "    If a merchant is provided, the query filters by that merchant.\n",
    "    If a spending category is provided, the query filters by that category.\n",
    "    If an SQL aggregation function is provided, the query aggregates the amount based on that function.\n",
    "    If no aggregation function is provided, this means that the user is asking for a set of transactions. Then the query returns the amount, spending category, and merchant.\n",
    "    \"\"\"\n",
    "\n",
    "    if sql_aggregation:\n",
    "        select_string = f\"{sql_aggregation}(amount) AS amount\"\n",
    "\n",
    "    else:\n",
    "        select_string = \"amount, transactions_view.spending_cat, transactions_view.merchant\"\n",
    "    \n",
    "    base_query = f\"\"\"\n",
    "        WITH transactions_view AS (\n",
    "        SELECT transaction_id, date, amount, def_merchants.spending_cat, def_merchants.merchant FROM transactions LEFT JOIN def_merchants ON transactions.merchant_id = def_merchants.merchant_id)\n",
    "        \n",
    "        SELECT {select_string}\n",
    "        FROM transactions_view \n",
    "        WHERE date BETWEEN ? AND ? \n",
    "        \"\"\"    \n",
    "\n",
    "    query_params = [start_date, end_date]\n",
    "\n",
    "    # Add merchant filter if provided\n",
    "    if merchant:\n",
    "        base_query += \" AND merchant = ?\"\n",
    "        query_params.append(merchant.upper())\n",
    "\n",
    "    # Add spending category filter if provided\n",
    "    if spending_category:\n",
    "        base_query += \" AND spending_cat = ?\"\n",
    "        query_params.append(spending_category.upper())\n",
    "    #print(base_query)\n",
    "    return base_query, query_params  # Return query string and parameters\n",
    "\n",
    "\n",
    "def call_database(sql_query, query_params):\n",
    "\n",
    "    \"\"\"\n",
    "    Call the database with the generated SQL query and return the results.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Connect to SQLite database\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute the query\n",
    "        cursor.execute(sql_query, query_params)\n",
    "\n",
    "        # Fetch all results\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Commit and close connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        #print(results)\n",
    "        return results  # Return the fetched query results\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        return None\n",
    "\n",
    "@timing_decorator\n",
    "def function_calling_db(user_query):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that:\n",
    "        1) Preprocess the user query\n",
    "        2) Extract parameters from the user query\n",
    "        3) Generate SQL query based on the extracted parameters\n",
    "        4) Call the database with the generated SQL query\n",
    "\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI()\n",
    "    log_error = []\n",
    "    # Get today's date in YYYY-MM-DD format\n",
    "    today_date = datetime(2025, 3, 4).strftime(\"%Y-%m-%d\")\n",
    "    day_of_week = datetime(2025, 3, 4).strftime(\"%A\")\n",
    "    \n",
    "    # 1) Preprocess the user query\n",
    "    preprocessed_user_query, preprocess_time = preprocess_user_query(user_query)\n",
    "\n",
    "    # 2) Extract parameters from the user query\n",
    "    prompt_enhanced = PromptTemplate.from_template(\"{preprocessed_user_query} (Today is {today_date}, {day_of_week})\")\n",
    "\n",
    "    prompt = prompt_enhanced.format( preprocessed_user_query = preprocessed_user_query, today_date=today_date, day_of_week=day_of_week)\n",
    "\n",
    "    # Call OpenAI API with function calling\n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You extract parameters from a user query\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        functions = function_calling,\n",
    "        function_call=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Check if function calling response is present\n",
    "    if response.choices[0].message.function_call:\n",
    "        function_args = json.loads(response.choices[0].message.function_call.arguments)\n",
    "        #print(function_args)\n",
    "\n",
    "        # Extract required parameters\n",
    "        start_date = function_args.get(\"start_date\")\n",
    "        end_date = function_args.get(\"end_date\")\n",
    "        merchant = function_args.get(\"merchant\")\n",
    "        if merchant:\n",
    "            spending_category = None\n",
    "            merchant = merchant.upper()\n",
    "        else:\n",
    "            spending_category = function_args.get(\"spending_category\")\n",
    "            if spending_category:\n",
    "                spending_category = spending_category.upper()\n",
    "        sql_aggregation = function_args.get(\"sql_aggregation\")\n",
    "\n",
    "        if spending_category not in list(CATEGORY_MAPPING.keys()) and spending_category is not None:\n",
    "            log_error.append('the extracted spending_category is not in the predefined list of spending categories')\n",
    "        if preprocess_time > preprocessing_time_threshold:\n",
    "            log_error.append(f'Worning: the preprocessing time is above {preprocessing_time_threshold} seconds')\n",
    "        \n",
    "\n",
    "        # 3) Call SQL query generator\n",
    "        sql_query, query_params = generate_sql_query(start_date, end_date, merchant, spending_category, sql_aggregation)\n",
    "\n",
    "        # 4) Call the database with the generated SQL query\n",
    "        \n",
    "        #print(sql_query, query_params)\n",
    "        db_results = call_database(sql_query, query_params)\n",
    "        if sql_aggregation:\n",
    "            df_results = pd.DataFrame(db_results, columns=['amount'])\n",
    "        else:\n",
    "            df_results = pd.DataFrame(db_results, columns=['amount', 'category', 'merchant'])\n",
    "        \n",
    "        if db_results:\n",
    "            result = df_results\n",
    "        else:\n",
    "            result = None\n",
    "\n",
    "        return preprocessed_user_query, start_date, end_date, merchant, spending_category, sql_aggregation, result, preprocess_time, log_error\n",
    "\n",
    "    \n",
    "    log_error.append(\"the query does not contain extractable fields\")\n",
    "    preprocessed_user_query, start_date, end_date, merchant, spending_category, sql_aggregation, result, preprocess_time = None, None, None, None, None, None, None, None\n",
    "\n",
    "    return preprocessed_user_query, start_date, end_date, merchant, spending_category, sql_aggregation, result, preprocess_time, log_error\n",
    "\n",
    "def return_response(user_query, print_results = False):\n",
    "    \"\"\"\n",
    "    Function that returns the response to the user query in a nicely formatted way.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()  # Start timing\n",
    "    \n",
    "    # Call the main function\n",
    "    (full_result, function_time) = function_calling_db(user_query)\n",
    "    preprocessed_user_query, start_date, end_date, merchant, spending_category, sql_aggregation, db_results, preprocess_time, log_error = full_result\n",
    "\n",
    "    if db_results is not None:\n",
    "        res = list(db_results.amount.values)\n",
    "    else:\n",
    "        res = db_results\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    total_execution_time = end_time - start_time  # Calculate execution time\n",
    "\n",
    "    if total_execution_time > 10:\n",
    "        log_error.append(\"Warning: the total execution time is above 10 seconds\")\n",
    "        \n",
    "    results_data = []\n",
    "    # Store results\n",
    "    results_data.append({\n",
    "        \"Original Query\": user_query,\n",
    "        \"Cleaned Query\": preprocessed_user_query,  # Store cleaned version\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"merchant\": merchant,\n",
    "        \"spending_category\": spending_category,\n",
    "        \"SQL_aggregation\": sql_aggregation,\n",
    "        \"Response\": res,\n",
    "        \"Preprocessing time (s)\": preprocess_time,\n",
    "        \"Total Time (s)\": total_execution_time,\n",
    "        \"Log errors\": log_error\n",
    "        }\n",
    "    )\n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    \n",
    "    if print_results:\n",
    "        if res is not None:\n",
    "            # Print the results\n",
    "            print(f\"\\n\\n\\nUser Query: {user_query}\\n\")\n",
    "            print(f\"Preprocessed Query: {preprocessed_user_query}\\n\")\n",
    "            print(f\"Start Date: {start_date}\")\n",
    "            print(f\"End Date: {end_date}\")\n",
    "            print(f\"Merchant: {merchant}\")\n",
    "            print(f\"Spending Category: {spending_category}\")\n",
    "            print(f\"SQL Aggregation: {sql_aggregation}\\n\")\n",
    "            print(f\"Query Results: {res}\\n\"),\n",
    "            print(f\"Preprocessing time (s): {preprocess_time}\"),\n",
    "            print(f\"Total Execution Time (s): {total_execution_time}\")\n",
    "            print(f\"Log errors: {log_error}\")\n",
    "        else:\n",
    "            print(\"No results found for the query.\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# For parallelization of the queries\n",
    "def process_query(user_query):\n",
    "    return return_response(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single query case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a query here:\n",
    "\n",
    "user_query = \"How much did I spend on groceries between January and June?\"\n",
    "user_query = \"how much did I spend on Feb 27th?\"\n",
    "\n",
    "# results are printed here:\n",
    "\n",
    "res = return_response(user_query, print_results = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple queries\n",
    "\n",
    "A test set is run in parallel, results are stored in a df (`df_testing_results`). \n",
    "A benchmark df (`benchmarks`) is imported and the results from `df_testing_results` are compared for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the testing df\n",
    "\n",
    "user_queries = [\"How much did I spend on groceries in the last 4 months?\",\n",
    "                \"What was the total spend at McDonald's last week?\", \n",
    "                \"What was the max I have spent on groceries in the last year?\",\n",
    "                \"How much did I spend on cinemas in the last year?\",\n",
    "                \"wht was my spending on fod in the last 3 months?\",                             # spelling mistakes and normalization\n",
    "                \"how much did I invest in Starling since January 1st, 2025?\",                   # alternative date formatting\n",
    "                \"how much did I invest since January 1st, 2025?\",                               # test that filters only by FINANCE \n",
    "                \"how many times I went to the cinema in the last 3 months?\",                    # test the count\n",
    "                \"What was the total spend at Tesco?\",                                           # test that you it works without a start date\n",
    "                \"What was the average spending at Weitrose?\",                                   # test the average and the spelling of a brand\n",
    "                \"What was the overall spending at Starling between 01/03/2025 and 11/03/2025?\", # testing the ambiguous date format\n",
    "                \"What was the average spending at Tesco between 13/01/2025 and 20/04/2025?\",    # testing the date range (DD-MM-AAAA)\n",
    "                \"what are all my transactions at Starling?\",                                    # testing that it returns the transactions\n",
    "                \"who is the president of the United States?\",                                   # testing that it does not return anything if the query is not related to transactions\n",
    "                 ] \n",
    "\n",
    "\n",
    "\n",
    "# Run queries in parallel\n",
    "df_testing_results = pd.DataFrame()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_query, user_queries))\n",
    "\n",
    "# Concatenate results\n",
    "#df_testing_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# ðŸ”¹ Silence the FutureWarning when using pd.concat\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    df_testing_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "# Look at the column PASSED TEST? to see if the test passed or failed. Look at Log errors for more information on errors.\n",
    "comparison_df = validation(df_testing_results)\n",
    "\n",
    "# percentage of passed test cases\n",
    "print(f\"\"\"*********\\nThe percentage of passed test is:\\n\\n {comparison_df['PASSED TEST?'].value_counts(normalize=True).iloc[0]*100:.0f}%\\n\n",
    "total execution time is {round(comparison_df['Total Time (s)'].mean(), 2)} seconds on average\\n\\n\"\"\")\n",
    "\n",
    "comparison_df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional notes on project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fast Inference**\n",
    "\n",
    "Currently, the response is processed in **about 4 seconds**, well below the **10-second limit**. The **preprocessing step** is the most time-consuming, while the SQL query itself is generated **almost instantly** since it is **statically created** from extracted features.\n",
    "\n",
    "> **Note:** Using an **LLM to generate the SQL query** would offer more **flexibility** but at the cost of **longer processing times** and potential **inconsistencies** due to hallucinations.\n",
    "\n",
    "The **database query execution** is **virtually instantaneous** since the system uses a **SQLite database** with only a few columns. However, on a **real database with millions of rows**, inference speed would depend on:  \n",
    "- **Network latency** (if the database is remote)  \n",
    "- **Database size**  \n",
    "\n",
    "### **Optimizing Query Efficiency**\n",
    "\n",
    "Currently, the SQL query:  \n",
    "1. **Creates a view** by **joining** the `transactions` and `merchant_id` tables.  \n",
    "2. **Applies filtering** using the `WHERE` clause.  \n",
    "\n",
    "However, **this approach is inefficient** when the user requests a summary for a **specific date range**. Since **`JOINs` are generally more expensive than `WHERE`**, a more efficient approach would be:  \n",
    "- **Filter by date first** in the `transactions` table (assuming it is much larger than `merchant_id`).  \n",
    "- **Perform the `JOIN` operation** only on the filtered subset.  \n",
    "\n",
    "This reduces the number of rows being joined, improving performance significantly.\n",
    "\n",
    "If inference is still a problem, multiple Databases could be used. For instance databases that are specific to a country to reduce the size of the DB itself (if a coutry does not have a TESCO there is no need to have it in the table for that coutry).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scalability**\n",
    "\n",
    "The process is modular, allowing each step to be individually improved. The AI task is divided into two main stagesâ€”preprocessing the user query and feature extractionâ€”to enhance consistency and optimize each task separately. \n",
    "\n",
    "For example, the feature extraction step could be executed multiple times in parallel to identify consistently detected features (which should be retained) and discard those that are infrequently identified.\n",
    "\n",
    "SQL operations are handled independently, ensuring flexibility in database selection. If a NoSQL database (e.g., MongoDB) is used, the AI-extracted features remain validâ€”it would simply require inserting them into a different precompiled format.\n",
    "\n",
    "#### **Handling LLM API Limits**\n",
    "LLM APIs enforce limits on the number of calls per day, typically measured in **Requests Per Minute (RPM)** or **Tokens Per Minute (TPM)**. If these limits are reached, the system should seamlessly switch to backup solutions, such as:\n",
    "\n",
    "- **Alternative Closed-Source LLMs**\n",
    "- **Open-Source LLMs**\n",
    "- **Dedicated Smaller LLMs**: A lightweight, fine-tuned model running on a small GPU cluster could handle specific tasks like grammar correction and feature extraction. Open-source models designed for such tasks would be ideal because:\n",
    "  - The task is generally simple and does not require complex reasoning.\n",
    "  - It involves processing a small number of tokens in both input and output.\n",
    "  \n",
    "- **Caching Mechanism**: User queries could be temporarily stored for a few days. If an identical query is submitted again, the cached response would be retrieved, bypassing redundant database calls.\n",
    "\n",
    "A process implemeting cache is diplayed below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image('processing_with_cache.png', dpi = 300, scale = 1.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Spending_Insights_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
